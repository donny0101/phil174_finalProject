<!DOCTYPE HTML>
<html>
    <head>
        <meta charset="utf-8">
        <title>Data Use Page</title>

        <link rel="stylesheet" type="text/css" href="css/agile.css">
        <style>
            #firstdiv {
                background: greenyellow;
                border: 20px;
                font-size: 1rem;
            }

            
            #header {
                background-color: black;
                padding: 20px;
            }
            #section {
                padding: 20px;
            }
            #firstdiv {
                border: 20px;
                font-size: 1rem;
            }

            #scarydiv{
                font-size: 2rem;
                border: 3px solid orange;
            }

            #understand {
                font-size: 2rem;
                color: red;
            }

            #header2 {
                font-size: 2rem;
                background-color: brown;
                border: 2px solid beige;
                margin: 1rem;
            }

            #header3 {
                font-size: 2rem;
                border: 2px;
                margin: 3px;
                background-color: bisque;
            }
        
        </style>
    </head>
   
   
       <body style="background-color: rebeccapurple">
        <h5 id="header3">Data Use </h5>

        <img src="phil174powerful.png">

        <h1>
            Explain how to interpret the outputs of the algorithm, perhaps in a context-sensitive manner
        </h1>

        <div>
            The algorithm used by the Dutch government to create risk profiles for individuals applying for child care benefits 
            resulted in individuals getting wrongly denied, leaving families destroyed as children were given up to foster care
            since their parents could not support them and even some committing suicde from the insane taxes handed out by the 
            government. Those that were denied were mainly minorities or low-income, disproportionately denying these groups 
            benefits based on unfair parameters. The Dutch Tax Authority provided data given by users when they first applied
            for childcare benefits to the model and then early into receiving benefits would get "evaluated" based on risk 
            factors determined by the algorithm. However, some individuals were given significantly high risk scores for factors
            such as race, nationality, and income. 
        </div>

        <h2>
            Identifies and explains at least two ways in which proper use of the model could result in increased 
            justice in appropriate contexts
        </h2>

        <div>
            If the model used by the Dutch government had worked as intended, there would have been a reduction in those 
            receiving child care benefits fraudulently. This would have resulted in more resources being allocated to the 
            families that actually needed the benefits. It is true that some individuals receiving benefits weren't actually
            in need and thus taking some essential resources away from those that actually need the help.
            Proper use of the model could have also resulted in families receiving benefits quicker as the algorithm would assess
            individuals much faster than a team of humans ever could. Automation is becoming increasingly more used in many areas
            like fast food and banking so childcare benefits seems like another likely application. 
        </div>

        <h1>
            Describes and explains at least two potential situations - contexts, uses, etc - in which the model should be 
            updated or revised (perhaps no longer using it) for ethical reasons
        </h1>

        <div>
            The first and most obvious case the model should have been updated would have been for those that were denied 
            benefits based on nationality/race. Denying anyone simply because of the color of their skin or where they came 
            from is obviously morally wrong. The fact that minorities scored riskier profiles simply because of race is quite 
            frightening. This was an inherit bias within the model that remained and was used for years and years causing a lot
            of pain for minorites getting denied simply because of their background.
            Another situation where the model needed to be updated would have been for all those families that 
            gave up their children to foster care. I wonder if this was because they were of low-income status and thus wrongly 
            denied benefits. 
        </div>

        <h2>
            For each of those situations, propose an ethical plan-of-response (such as collecting more data, ceasing model use,
             etc) if we find ourselves in that problematic situation
        </h2>

        <div>
            <p>
            I think in order to address the nationality/race bias in the program, programmers could consider making the
            algorithm race/nationality blind or having a third party review of how the model is performing in underserved 
            communities could make the model more progresssive. The way the current model handles race is completely wrong 
            and is the source of why this whole scandal began. However race and nationality are currently implemented in the model 
            should either be removed entirely or significantly turned down as we have seen from the current results of the system that 
            the model is not working as intended. 
            </p>
            <p>
            In assisting those with low-income, the Dutch government should have again had some sort of “checks and balances” 
            to the system to analyze the data on who was getting denied benefits and why some profiles were considered riskier than others 
            to adjust the algorithm as necessary. In the previous pages I have suggested a few ways this could have been done. I believe
            that the most important of all of these would have been the creation of an oversight team to monitor how the the model was 
            performing on the real data. If there had been any kind oversight most of the suffering caused by this predictive model could 
            have been avoided. With an oversight team in place to monitor the model, we can look at several several areas to see how to 
            respond to potential unethical or incorrect uses of the model. One suggestion I have is the team routinely manuallly at the 
            fradulent profiles being detected by the system - every week or month - to see not only the accuraccy of the algorithm but to 
            make adjustments in the how profiles are evaluated to avoid making an error and denying someone that really needs the benefits. 
            Another suggestion would be to add several safeguards within the software itself to keep track of the kinds of profiles being 
            flagged as fradulent. The model could go through the denied profiles and find the similarities in the backgrounds of these 
            individuals, eventually alerting the team if there is a significantly high percentage of a certain group or factor leading
            profiles to be denied. The software could also then stop adding these profiles with these backgrounds to the fradulent list
            and instead be put on some revealation list where the team or another model can go through and see these groups are being flagged
            as risky. 
            </p>
        </div>

        


        <div id="firstdiv">
            <a href="/product.html">DATA COLLECTION PAGE </a> 
        </div>
        <div id="firstdiv">
        <div id="firstdiv">
            <a href="/KVP.html">DATA ANALYSIS PAGE </a> 
        </div>
        <div id="firstdiv">
            <a href="/IA6.html">FUTURE ENGINEERS </a> 
        </div>
            <a href="/">HOME PAGE </a> 
        </div>
        
       </body>
</html>